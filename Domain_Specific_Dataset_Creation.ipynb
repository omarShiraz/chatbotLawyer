{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/omarShiraz/chatbotLawyer/blob/main/Domain_Specific_Dataset_Creation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4L5Ty6u38rT"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --verbose git+https://github.com/omarShiraz/Questgen.ai.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FUXJxwqJ3_tA"
      },
      "outputs": [],
      "source": [
        "!pip install fitz\n",
        "!pip install PyMuPDF\n",
        "!pip install transformers\n",
        "!pip install --upgrade numpy\n",
        "!pip install spaCy==2.3.3\n",
        "!pip install --quiet git+https://github.com/boudinfl/pke.git\n",
        "!python -m nltk.downloader universal_tagset\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rO8bFx44VWN"
      },
      "source": [
        "## **Restart runtime before continuing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2R-b20B6OFj"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/explosion/sense2vec/releases/download/v1.0.0/s2v_reddit_2015_md.tar.gz\n",
        "!tar -xvf  s2v_reddit_2015_md.tar.gz\n",
        "!ls s2v_old"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FTRjmMz4092"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from pprint import pprint\n",
        "from Questgen import main\n",
        "qg = main.QGen()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "QFsqbIdJrlGv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ama9Ef-T9nO_"
      },
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "import pandas as pd\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Function to extract text from a PDF file\n",
        "def extract_text_from_pdf(pdf_file):\n",
        "    doc = fitz.open(pdf_file)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "# Function to chunk text into pieces of a specified size\n",
        "def chunk_text(text, chunk_size=4000):\n",
        "    chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]\n",
        "    return chunks\n",
        "\n",
        "# Function to preprocess text (you can customize this)\n",
        "def preprocess_text(text):\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Specify the path to the zip file containing multiple PDFs\n",
        "zip_file_path = '/content/LawData1.zip'\n",
        "\n",
        "# Create a DataFrame to store the results\n",
        "results_df = pd.DataFrame(columns=['output', 'instruction'])\n",
        "\n",
        "# Extract PDFs from the zip file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall()\n",
        "\n",
        "# List all PDF files in the current working directory\n",
        "pdf_files = [file for file in os.listdir() if file.lower().endswith('.pdf')]\n",
        "# Loop through each PDF file\n",
        "for pdf_file_path in pdf_files:\n",
        "    # Extract text from the PDF\n",
        "    pdf_text = extract_text_from_pdf(pdf_file_path)\n",
        "\n",
        "    # Preprocess the text\n",
        "    cleaned_text = preprocess_text(pdf_text)\n",
        "\n",
        "    # Chunk the text into 512-token pieces since questgen only excepts 512 tokens per input\n",
        "    text_chunks = chunk_text(cleaned_text, chunk_size=512)\n",
        "\n",
        "    payload = {\"input_text\": \"\"}\n",
        "\n",
        "    # Loop through chunks and make predictions\n",
        "    for chunk in text_chunks:\n",
        "        try:\n",
        "            payload[\"input_text\"] = chunk\n",
        "            outputs = qg.predict_shortq(payload)\n",
        "\n",
        "            # Check the structure of the outputs dictionary\n",
        "            if 'questions' in outputs:\n",
        "                question_list = outputs['questions']\n",
        "            elif 'your_custom_key' in outputs:\n",
        "                question_list = outputs['your_custom_key']\n",
        "            else:\n",
        "                print(\"Unexpected structure in the 'outputs' dictionary. Check the structure and update the code.\")\n",
        "                continue\n",
        "\n",
        "            # Iterate through the extracted questions and contexts\n",
        "            for item in question_list:\n",
        "                question = item.get('Question', '')\n",
        "                context = item.get('context', '')\n",
        "\n",
        "                # Check if question and context are non-empty before processing\n",
        "                if question and context:\n",
        "                    # Include context in the instruction field\n",
        "                    instruction_text = context\n",
        "\n",
        "                    # Append the results to the DataFrame\n",
        "                    results_df = results_df.append({\n",
        "                        \"input\": question,\n",
        "                        \"instruction\": instruction_text\n",
        "                    }, ignore_index=True)\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Error processing chunk: {e}\")\n",
        "            continue\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "results_df.to_csv('LawDataset4.csv', index=False)\n",
        "\n",
        "# Print the generated DataFrame\n",
        "print(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install Datasets library to load the dataset from hugging face into the Google Colab Notebook.\n",
        "#Install Transformers library to import the Autotokenizer this will convert the raw text into tokens\n",
        "#Install Sentence Transformers Library to download the Embedding Model\n",
        "!pip install -q datasets transformers sentence_transformers faiss-gpu"
      ],
      "metadata": {
        "id": "yB9XHXDQsZfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Set the Hugging Face Token**"
      ],
      "metadata": {
        "id": "VL01RpexsdUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"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\""
      ],
      "metadata": {
        "id": "YVI9TXF-scL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Load the Dataset**"
      ],
      "metadata": {
        "id": "s0XdMaxLslpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"zoom12/SriLankaLaw\")\n",
        "dataset"
      ],
      "metadata": {
        "id": "KxzO47W0slVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Load CSV File**"
      ],
      "metadata": {
        "id": "WPex96PNssGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"csv\", data_files=\"/content/LawDataset4.csv\") #Change the file name to output csv file or any name you prefer\n",
        "dataset"
      ],
      "metadata": {
        "id": "0LECRZJlsqi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Map Chat templates**"
      ],
      "metadata": {
        "id": "wlOZa7pcs2l_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_template(example):\n",
        "    example[\"instruction\"] = f\"### Instruction:\\n{example['instruction']}\\n\\n### Response:\\n\"\n",
        "    return example\n",
        "\n",
        "dataset = dataset.map(chat_template)"
      ],
      "metadata": {
        "id": "B7NiWIsBs7TA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Push the Dataset to Hugging Face Hub**"
      ],
      "metadata": {
        "id": "payQs7vJtBDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HF_TOKEN\"] = \"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\""
      ],
      "metadata": {
        "id": "aEwsL9sPtDSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.push_to_hub(\"zoom12/SriLankaLaw\")"
      ],
      "metadata": {
        "id": "jpdrECHMtFIZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}