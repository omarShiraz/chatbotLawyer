{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiIOhb7iVC3J"
      },
      "source": [
        "# Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PucJwfbhVC3L"
      },
      "source": [
        "This tutorial will demonstrate how to train, evaluate, and test three types of models for Question-Answering -\n",
        "1. BERT-like models for Extractive Question-Answering\n",
        "2. Sequence-to-Sequence (S2S) models for Generative Question-Answering (ex. T5/BART-like)\n",
        "3. GPT-like models for Generative Question-Answering\n",
        "\n",
        "## Task Description\n",
        "\n",
        "- Given a context and a natural language query, we want to generate an answer for the query\n",
        "- Depending on how the answer is generated, the task can be broadly divided into two types:\n",
        "    1. Extractive Question Answering\n",
        "    2. Generative Question Answering\n",
        "\n",
        "\n",
        "### Extractive Question-Answering with BERT-like models\n",
        "\n",
        "Given a question and a context, both in natural language, predict the span within the context with a start and end position which indicates the answer to the question.\n",
        "For every word in our training dataset we’re going to predict:\n",
        "- likelihood this word is the start of the span\n",
        "- likelihood this word is the end of the span\n",
        "\n",
        "We are using a BERT encoder with 2 span prediction heads for predicting start and end position of the answer. The span predictions are token classifiers consisting of a single linear layer.\n",
        "\n",
        "### Generative Question-Answering with S2S and GPT-like models\n",
        "\n",
        "Given a question and a context, both in natural language, generate an answer for the question. Unlike the BERT-like models, there is no constraint that the answer should be a span within the context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpX0w2PtVC3M"
      },
      "source": [
        "# Installing NeMo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72XWYFQYVC3M"
      },
      "source": [
        "You can run either this notebook locally (if you have all the dependencies and a GPU) or on Google Colab.\n",
        "\n",
        "Instructions for setting up Colab are as follows:\n",
        "1. Open a new Python 3 notebook.\n",
        "2. Import this notebook from GitHub (File -> Upload Notebook -> \"GITHUB\" tab -> copy/paste GitHub URL)\n",
        "3. Connect to an instance with a GPU (Runtime -> Change runtime type -> select \"GPU\" for hardware accelerator)\n",
        "4. Run the cell below to set up dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_xQBtr0KVC3M"
      },
      "outputs": [],
      "source": [
        "BRANCH = 'r1.20.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9R1D6W58VC3N",
        "outputId": "62188a1a-00b3-40f8-d25d-2f0dbac88251",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mDEPRECATION: git+https://github.com/NVIDIA/NeMo.git@r1.20.0#egg=nemo_toolkit[nlp] contains an egg fragment with a non-PEP 508 name pip 25.0 will enforce this behaviour change. A possible replacement is to use the req @ url syntax, and remove the egg fragment. Discussion can be found at https://github.com/pypa/pip/issues/11617\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting nemo_toolkit[nlp]\n",
            "  Cloning https://github.com/NVIDIA/NeMo.git (to revision r1.20.0) to /tmp/pip-install-ljoenbw0/nemo-toolkit_9dbc5910f3fe42e4ae3a40c7da542290\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/NVIDIA/NeMo.git /tmp/pip-install-ljoenbw0/nemo-toolkit_9dbc5910f3fe42e4ae3a40c7da542290\n",
            "  Running command git checkout -b r1.20.0 --track origin/r1.20.0\n",
            "  Switched to a new branch 'r1.20.0'\n",
            "  Branch 'r1.20.0' set up to track remote branch 'r1.20.0' from 'origin'.\n",
            "  Resolved https://github.com/NVIDIA/NeMo.git to commit 2baef811f21372c3340dd2d82635d2377e78a660\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface-hub (from nemo_toolkit[nlp])\n",
            "  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (0.56.4)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (1.23.5)\n",
            "Collecting onnx>=1.7.0 (from nemo_toolkit[nlp])\n",
            "  Downloading onnx-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (2.8.2)\n",
            "Collecting ruamel.yaml (from nemo_toolkit[nlp])\n",
            "  Downloading ruamel.yaml-0.17.35-py3-none-any.whl (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.9/112.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (1.2.2)\n",
            "Collecting setuptools==65.5.1 (from nemo_toolkit[nlp])\n",
            "  Downloading setuptools-65.5.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (2.13.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (1.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (2.0.1+cu118)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (4.66.1)\n",
            "Collecting wget (from nemo_toolkit[nlp])\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (1.15.0)\n",
            "Collecting boto3 (from nemo_toolkit[nlp])\n",
            "  Downloading boto3-1.28.62-py3-none-any.whl (135 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops (from nemo_toolkit[nlp])\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu (from nemo_toolkit[nlp])\n",
            "  Downloading faiss_cpu-1.7.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasttext (from nemo_toolkit[nlp])\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flask-restful (from nemo_toolkit[nlp])\n",
            "  Downloading Flask_RESTful-0.3.10-py2.py3-none-any.whl (26 kB)\n",
            "Collecting ftfy (from nemo_toolkit[nlp])\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (4.6.6)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (3.9.0)\n",
            "Collecting ijson (from nemo_toolkit[nlp])\n",
            "  Downloading ijson-3.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (111 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.8/111.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (0.42.1)\n",
            "Collecting markdown2 (from nemo_toolkit[nlp])\n",
            "  Downloading markdown2-2.4.10-py2.py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: matplotlib>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (3.7.1)\n",
            "Collecting megatron-core==0.2.0 (from nemo_toolkit[nlp])\n",
            "  Downloading megatron_core-0.2.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (3.8.1)\n",
            "Collecting opencc (from nemo_toolkit[nlp])\n",
            "  Downloading OpenCC-1.1.6-cp310-cp310-manylinux1_x86_64.whl (778 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.3/778.3 kB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pangu (from nemo_toolkit[nlp])\n",
            "  Downloading pangu-4.0.6.1-py3-none-any.whl (6.4 kB)\n",
            "Collecting rapidfuzz (from nemo_toolkit[nlp])\n",
            "  Downloading rapidfuzz-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge-score (from nemo_toolkit[nlp])\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu[ja] (from nemo_toolkit[nlp])\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentence-transformers (from nemo_toolkit[nlp])\n",
            "  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting hydra-core<1.3,>=1.2.0 (from nemo_toolkit[nlp])\n",
            "  Downloading hydra_core-1.2.0-py3-none-any.whl (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.1/151.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.3,>=2.2 (from nemo_toolkit[nlp])\n",
            "  Downloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning<=1.9.4,>=1.9.0 (from nemo_toolkit[nlp])\n",
            "  Downloading pytorch_lightning-1.9.4-py3-none-any.whl (827 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m827.8/827.8 kB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchmetrics>=0.11.0 (from nemo_toolkit[nlp])\n",
            "  Downloading torchmetrics-1.2.0-py3-none-any.whl (805 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m805.2/805.2 kB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers>=4.0.1 (from nemo_toolkit[nlp])\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb (from nemo_toolkit[nlp])\n",
            "  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting webdataset<=0.1.62,>=0.1.48 (from nemo_toolkit[nlp])\n",
            "  Downloading webdataset-0.1.62-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (7.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (1.5.3)\n",
            "Requirement already satisfied: pydantic<2 in /usr/local/lib/python3.10/dist-packages (from nemo_toolkit[nlp]) (1.10.13)\n",
            "Collecting sacremoses>=0.0.43 (from nemo_toolkit[nlp])\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sentencepiece<1.0.0 (from nemo_toolkit[nlp])\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting youtokentome>=1.0.5 (from nemo_toolkit[nlp])\n",
            "  Downloading youtokentome-1.0.6.tar.gz (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core<1.3,>=1.2.0->nemo_toolkit[nlp])\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core<1.3,>=1.2.0->nemo_toolkit[nlp]) (23.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo_toolkit[nlp]) (1.1.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo_toolkit[nlp]) (0.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo_toolkit[nlp]) (4.43.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo_toolkit[nlp]) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo_toolkit[nlp]) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.2->nemo_toolkit[nlp]) (3.1.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->nemo_toolkit[nlp]) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->nemo_toolkit[nlp]) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.6.5->nemo_toolkit[nlp]) (2023.6.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.3,>=2.2->nemo_toolkit[nlp]) (6.0.1)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.7.0->nemo_toolkit[nlp]) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.7.0->nemo_toolkit[nlp]) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->nemo_toolkit[nlp]) (1.16.0)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[nlp]) (2023.6.0)\n",
            "Collecting lightning-utilities>=0.6.0.post0 (from pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[nlp])\n",
            "  Downloading lightning_utilities-0.9.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[nlp]) (3.12.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[nlp]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[nlp]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[nlp]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->nemo_toolkit[nlp]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->nemo_toolkit[nlp]) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->nemo_toolkit[nlp]) (17.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.0.1->nemo_toolkit[nlp]) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers>=4.0.1->nemo_toolkit[nlp])\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers>=4.0.1->nemo_toolkit[nlp])\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting braceexpand (from webdataset<=0.1.62,>=0.1.48->nemo_toolkit[nlp])\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Collecting botocore<1.32.0,>=1.31.62 (from boto3->nemo_toolkit[nlp])\n",
            "  Downloading botocore-1.31.62-py3-none-any.whl (11.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m121.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3->nemo_toolkit[nlp])\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.8.0,>=0.7.0 (from boto3->nemo_toolkit[nlp])\n",
            "  Downloading s3transfer-0.7.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pybind11>=2.2 (from fasttext->nemo_toolkit[nlp])\n",
            "  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "Collecting aniso8601>=0.82 (from flask-restful->nemo_toolkit[nlp])\n",
            "  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.10/dist-packages (from flask-restful->nemo_toolkit[nlp]) (2.2.5)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from flask-restful->nemo_toolkit[nlp]) (2023.3.post1)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->nemo_toolkit[nlp]) (0.2.8)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->nemo_toolkit[nlp]) (4.11.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->nemo_toolkit[nlp]) (0.39.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score->nemo_toolkit[nlp]) (1.4.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->nemo_toolkit[nlp])\n",
            "  Downloading ruamel.yaml.clib-0.2.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (526 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m526.7/526.7 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu[ja]->nemo_toolkit[nlp])\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu[ja]->nemo_toolkit[nlp]) (0.9.0)\n",
            "Collecting colorama (from sacrebleu[ja]->nemo_toolkit[nlp])\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu[ja]->nemo_toolkit[nlp]) (4.9.3)\n",
            "Collecting mecab-python3==1.0.5 (from sacrebleu[ja]->nemo_toolkit[nlp])\n",
            "  Downloading mecab_python3-1.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (581 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m581.1/581.1 kB\u001b[0m \u001b[31m55.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ipadic<2.0,>=1.0 (from sacrebleu[ja]->nemo_toolkit[nlp])\n",
            "  Downloading ipadic-1.0.0.tar.gz (13.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m119.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->nemo_toolkit[nlp]) (1.11.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->nemo_toolkit[nlp]) (3.2.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->nemo_toolkit[nlp]) (0.15.2+cu118)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[nlp]) (1.59.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[nlp]) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[nlp]) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[nlp]) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[nlp]) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[nlp]) (3.0.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard->nemo_toolkit[nlp]) (0.41.2)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->nemo_toolkit[nlp])\n",
            "  Downloading GitPython-3.1.37-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.0/190.0 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo_toolkit[nlp]) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb->nemo_toolkit[nlp])\n",
            "  Downloading sentry_sdk-1.31.0-py2.py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.8/224.8 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->nemo_toolkit[nlp])\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb->nemo_toolkit[nlp])\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb->nemo_toolkit[nlp])\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->nemo_toolkit[nlp]) (1.4.4)\n",
            "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.32.0,>=1.31.62->boto3->nemo_toolkit[nlp]) (2.0.6)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask>=0.8->flask-restful->nemo_toolkit[nlp]) (2.1.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>2021.06.0->pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[nlp]) (3.8.5)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->nemo_toolkit[nlp])\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->nemo_toolkit[nlp]) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->nemo_toolkit[nlp]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->nemo_toolkit[nlp]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard->nemo_toolkit[nlp]) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->nemo_toolkit[nlp]) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.1->nemo_toolkit[nlp]) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.1->nemo_toolkit[nlp]) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.1->nemo_toolkit[nlp]) (2023.7.22)\n",
            "Collecting huggingface-hub (from nemo_toolkit[nlp])\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->nemo_toolkit[nlp]) (2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.0.1->nemo_toolkit[nlp]) (1.7.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->nemo_toolkit[nlp]) (1.3.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[nlp]) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[nlp]) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[nlp]) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[nlp]) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[nlp]) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>2021.06.0->pytorch-lightning<=1.9.4,>=1.9.0->nemo_toolkit[nlp]) (1.3.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->nemo_toolkit[nlp])\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->nemo_toolkit[nlp]) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard->nemo_toolkit[nlp]) (3.2.2)\n",
            "Building wheels for collected packages: antlr4-python3-runtime, sacremoses, youtokentome, fasttext, nemo_toolkit, rouge-score, sentence-transformers, wget, ipadic, pathtools\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=94f10b558e8ec33f0c81a30590178dc28fe8e1e7b662377dfd94a337244a5aba\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=0b66671c8adbb434f6f03a2cff36eb6d1d79e1781c54b6be0e58e7a1a4aca294\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "  Building wheel for youtokentome (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for youtokentome: filename=youtokentome-1.0.6-cp310-cp310-linux_x86_64.whl size=1951074 sha256=472c8deda2408270a765bda0cd65581dc642ceb3256793acdc4ffd9019bd9ef3\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/85/f8/301d2ba45f43f30bed2fe413efa760bc726b8b660ed9c2900c\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4199772 sha256=bb1b2d2d9661b1387bcdd4457dc50b919d5f12e06e5edef97536412b0704a664\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "  Building wheel for nemo_toolkit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nemo_toolkit: filename=nemo_toolkit-1.20.0-py3-none-any.whl size=2464994 sha256=7595aed47fd547e94f3bbbc88ac01d819fedfe03bacbf48a5ceea46de6aa3697\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-cbhadmg7/wheels/b5/34/43/cc084cf170a827b8e1e1a669ce1ffd243f3ca76cf086fb0ffe\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24932 sha256=063661bf8647cacba9f3aa1cfc92492b59d1bde056ecb5f1eef04040a378c364\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125923 sha256=5c7e1e19892539e49cf738b30471c40312b5a42250460e3c8cf6280f7b504613\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=f3a650360e3e4ad9cffbc52fadc023e496c7a38bdfc1fd978e5178944d3262e1\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\n",
            "  Building wheel for ipadic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipadic: filename=ipadic-1.0.0-py3-none-any.whl size=13556703 sha256=160573ecde6e97ad1d8f00208089b7e60ee7b844e96ead0f6a31185e860730a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/ea/e3/2f6e0860a327daba3b030853fce4483ed37468bbf1101c59c3\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=c82a93d46df14c79da06dca50137dccf65c2b3a75411a61296c4379bec820446\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built antlr4-python3-runtime sacremoses youtokentome fasttext nemo_toolkit rouge-score sentence-transformers wget ipadic pathtools\n",
            "Installing collected packages: wget, sentencepiece, pathtools, pangu, opencc, mecab-python3, ipadic, ijson, faiss-cpu, braceexpand, antlr4-python3-runtime, aniso8601, youtokentome, webdataset, smmap, setuptools, setproctitle, sentry-sdk, safetensors, sacremoses, ruamel.yaml.clib, rapidfuzz, pybind11, portalocker, onnx, omegaconf, markdown2, lightning-utilities, jmespath, ftfy, einops, docker-pycreds, colorama, sacrebleu, ruamel.yaml, rouge-score, hydra-core, huggingface-hub, gitdb, fasttext, botocore, tokenizers, s3transfer, GitPython, flask-restful, wandb, transformers, boto3, torchmetrics, sentence-transformers, pytorch-lightning, nemo_toolkit, megatron-core\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "cvxpy 1.3.2 requires setuptools>65.5.1, but you have setuptools 65.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.37 aniso8601-9.0.1 antlr4-python3-runtime-4.9.3 boto3-1.28.62 botocore-1.31.62 braceexpand-0.1.7 colorama-0.4.6 docker-pycreds-0.4.0 einops-0.7.0 faiss-cpu-1.7.4 fasttext-0.9.2 flask-restful-0.3.10 ftfy-6.1.1 gitdb-4.0.10 huggingface-hub-0.17.3 hydra-core-1.2.0 ijson-3.2.3 ipadic-1.0.0 jmespath-1.0.1 lightning-utilities-0.9.0 markdown2-2.4.10 mecab-python3-1.0.5 megatron-core-0.2.0 nemo_toolkit-1.20.0 omegaconf-2.2.3 onnx-1.14.1 opencc-1.1.6 pangu-4.0.6.1 pathtools-0.1.2 portalocker-2.8.2 pybind11-2.11.1 pytorch-lightning-1.9.4 rapidfuzz-3.4.0 rouge-score-0.1.2 ruamel.yaml-0.17.35 ruamel.yaml.clib-0.2.8 s3transfer-0.7.0 sacrebleu-2.3.1 sacremoses-0.0.53 safetensors-0.4.0 sentence-transformers-2.2.2 sentencepiece-0.1.99 sentry-sdk-1.31.0 setproctitle-1.3.3 setuptools-65.5.1 smmap-5.0.1 tokenizers-0.14.1 torchmetrics-1.2.0 transformers-4.34.0 wandb-0.15.12 webdataset-0.1.62 wget-3.2 youtokentome-1.0.6\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install git+https://github.com/NVIDIA/NeMo.git@$BRANCH#egg=nemo_toolkit[nlp]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fof5-57iVC3N"
      },
      "source": [
        "# Imports and constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KqKD-wReVC3O",
        "outputId": "0dcb1cbf-cfae-4b76-c7ff-2923d7445a4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Global seed set to 42\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import wget\n",
        "import gc\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from nemo.collections.nlp.models.question_answering.qa_bert_model import BERTQAModel\n",
        "from nemo.collections.nlp.models.question_answering.qa_gpt_model import GPTQAModel\n",
        "from nemo.collections.nlp.models.question_answering.qa_s2s_model import S2SQAModel\n",
        "from nemo.utils.exp_manager import exp_manager\n",
        "\n",
        "pl.seed_everything(42)\n",
        "gc.disable()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xhPr9Jf_VC3O"
      },
      "outputs": [],
      "source": [
        "# set the following paths\n",
        "DATA_DIR = \"/content/Data\" # directory for storing datasets\n",
        "WORK_DIR = \"/content/Model\" # directory for storing trained models, logs, additionally downloaded scripts\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(WORK_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWymW8e0VC3O"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YhKTkuXVC3P"
      },
      "source": [
        "The model is defined in a config file which declares multiple important sections:\n",
        "- **model**: All arguments that will relate to the Model - language model, span prediction, optimizer and schedulers, datasets and any other related information\n",
        "- **trainer**: Any argument to be passed to PyTorch Lightning\n",
        "- **exp_manager**: All arguments used for setting up the experiment manager - target directory, name, logger information\n",
        "\n",
        "We will download the default config file provided at `NeMo/examples/nlp/question_answering/conf/qa_conf.yaml` and edit necessary values for training different models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WOIWJqQ0VC3P",
        "outputId": "cb916175-797a-4daf-96a0-f4fa460db516",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading config file...\n"
          ]
        }
      ],
      "source": [
        "# download the model's default configuration file\n",
        "config_dir = WORK_DIR + '/conf/'\n",
        "os.makedirs(config_dir, exist_ok=True)\n",
        "if not os.path.exists(config_dir + \"qa_conf.yaml\"):\n",
        "    print('Downloading config file...')\n",
        "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/question_answering/conf/qa_conf.yaml', config_dir)\n",
        "else:\n",
        "    print ('config file already exists')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cvD-gv-FVC3P",
        "outputId": "51061f52-fb1a-4259-cf91-324e13a3b598",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Model/conf/qa_conf.yaml\n",
            "Default Config - \n",
            "\n",
            "pretrained_model: null\n",
            "do_training: true\n",
            "trainer:\n",
            "  devices:\n",
            "  - 0\n",
            "  num_nodes: 1\n",
            "  max_epochs: 3\n",
            "  max_steps: -1\n",
            "  accumulate_grad_batches: 1\n",
            "  gradient_clip_val: 1.0\n",
            "  precision: 16\n",
            "  accelerator: gpu\n",
            "  log_every_n_steps: 5\n",
            "  val_check_interval: 1.0\n",
            "  resume_from_checkpoint: null\n",
            "  num_sanity_val_steps: 0\n",
            "  enable_checkpointing: false\n",
            "  logger: false\n",
            "  strategy: ddp\n",
            "model:\n",
            "  tensor_model_parallel_size: 1\n",
            "  nemo_path: null\n",
            "  library: huggingface\n",
            "  save_model: false\n",
            "  tokens_to_generate: 32\n",
            "  dataset:\n",
            "    version_2_with_negative: true\n",
            "    doc_stride: 128\n",
            "    max_query_length: 64\n",
            "    max_seq_length: 512\n",
            "    max_answer_length: 30\n",
            "    use_cache: false\n",
            "    do_lower_case: true\n",
            "    check_if_answer_in_context: true\n",
            "    keep_doc_spans: all\n",
            "    null_score_diff_threshold: 0.0\n",
            "    n_best_size: 20\n",
            "    num_workers: 1\n",
            "    pin_memory: false\n",
            "    drop_last: false\n",
            "  train_ds:\n",
            "    file: null\n",
            "    batch_size: 24\n",
            "    shuffle: true\n",
            "    num_samples: -1\n",
            "    num_workers: ${model.dataset.num_workers}\n",
            "    drop_last: ${model.dataset.drop_last}\n",
            "    pin_memory: ${model.dataset.pin_memory}\n",
            "  validation_ds:\n",
            "    file: null\n",
            "    batch_size: 24\n",
            "    shuffle: false\n",
            "    num_samples: -1\n",
            "    num_workers: ${model.dataset.num_workers}\n",
            "    drop_last: ${model.dataset.drop_last}\n",
            "    pin_memory: ${model.dataset.pin_memory}\n",
            "  test_ds:\n",
            "    file: null\n",
            "    batch_size: 24\n",
            "    shuffle: false\n",
            "    num_samples: -1\n",
            "    num_workers: ${model.dataset.num_workers}\n",
            "    drop_last: ${model.dataset.drop_last}\n",
            "    pin_memory: ${model.dataset.pin_memory}\n",
            "  language_model:\n",
            "    pretrained_model_name: bert-base-uncased\n",
            "    lm_checkpoint: null\n",
            "    config_file: null\n",
            "    config: null\n",
            "  token_classifier:\n",
            "    num_layers: 1\n",
            "    dropout: 0.0\n",
            "    num_classes: 2\n",
            "    activation: relu\n",
            "    log_softmax: false\n",
            "    use_transformer_init: true\n",
            "  tokenizer:\n",
            "    tokenizer_name: ${model.language_model.pretrained_model_name}\n",
            "    vocab_file: null\n",
            "    tokenizer_model: null\n",
            "    special_tokens: null\n",
            "  optim:\n",
            "    name: adamw\n",
            "    lr: 5.0e-05\n",
            "    betas:\n",
            "    - 0.9\n",
            "    - 0.999\n",
            "    weight_decay: 0.0\n",
            "    sched:\n",
            "      name: SquareRootAnnealing\n",
            "      warmup_steps: null\n",
            "      warmup_ratio: 0.0\n",
            "      last_epoch: -1\n",
            "      monitor: val_loss\n",
            "      reduce_on_plateau: false\n",
            "exp_manager:\n",
            "  exp_dir: null\n",
            "  name: QnA\n",
            "  create_wandb_logger: false\n",
            "  wandb_logger_kwargs:\n",
            "    name: ???\n",
            "    project: QnA\n",
            "  create_tensorboard_logger: true\n",
            "  create_checkpoint_callback: true\n",
            "  resume_if_exists: false\n",
            "  resume_ignore_no_checkpoint: false\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# this will print the entire default config of the model\n",
        "config_path = f'{WORK_DIR}/conf/qa_conf.yaml'\n",
        "print(config_path)\n",
        "config = OmegaConf.load(config_path)\n",
        "print(\"Default Config - \\n\")\n",
        "print(OmegaConf.to_yaml(config))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E08e-ItPVC3P"
      },
      "source": [
        "# Training and testing models on SQuAD v2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xn022MsKVC3Q"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c356CGL1VC3Q"
      },
      "source": [
        "For this example, we are going to download the [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) dataset to showcase how to do training and inference. There are two datasets, SQuAD1.0 and SQuAD2.0. SQuAD 1.1, the previous version of the SQuAD dataset, contains 100,000+ question-answer pairs on 500+ articles. SQuAD2.0 dataset combines the 100,000 questions in SQuAD1.1 with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gaju1h_bVC3Q"
      },
      "source": [
        "To download both datasets, we use `NeMo/examples/nlp/question_answering/get_squad.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nb840_bZVC3Q",
        "outputId": "4bf94e03-2367-4912-b67a-ccc244b9f90b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading get_squad.py...\n"
          ]
        }
      ],
      "source": [
        "# download get_squad.py script to download and preprocess the SQuAD data\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "if not os.path.exists(WORK_DIR + '/get_squad.py'):\n",
        "    print('Downloading get_squad.py...')\n",
        "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/question_answering/get_squad.py', WORK_DIR)\n",
        "else:\n",
        "    print ('get_squad.py already exists')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "sOgY0tRzVC3Q",
        "outputId": "3af68bc5-9ca8-478e-d3b8-71a410a43e66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-10-10 17:12:34 get_squad:66] /content/Data\n",
            "[NeMo I 2023-10-10 17:12:34 get_squad:47] Downloading: https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json\n",
            "[NeMo I 2023-10-10 17:12:36 get_squad:47] Downloading: https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json\n",
            "[NeMo I 2023-10-10 17:12:37 get_squad:47] Downloading: https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json\n",
            "[NeMo I 2023-10-10 17:12:40 get_squad:47] Downloading: https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json\n"
          ]
        }
      ],
      "source": [
        "# download and preprocess the data\n",
        "!python $WORK_DIR/get_squad.py --destDir $DATA_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nprGkyvRVC3Q"
      },
      "source": [
        "After execution of the above cell, your data folder will contain a subfolder \"squad\" the following four files for training and evaluation\n",
        "\n",
        "```\n",
        "squad  \n",
        "│\n",
        "└───v1.1\n",
        "│   │ -  train-v1.1.json\n",
        "│   │ -  dev-v1.1.json\n",
        "│\n",
        "└───v2.0\n",
        "    │ -  train-v2.0.json\n",
        "    │ -  dev-v2.0.json\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GX0KWQXKVC3Q",
        "outputId": "3a2f1423-3bb0-48d1-9df1-c805e166eddb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/Data/squad:\n",
            "v1.1  v2.0\n",
            "\n",
            "/content/Data/squad/v1.1:\n",
            "dev-v1.1.json  train-v1.1.json\n",
            "\n",
            "/content/Data/squad/v2.0:\n",
            "dev-v2.0.json  train-v2.0.json\n"
          ]
        }
      ],
      "source": [
        "!ls -LR {DATA_DIR}/squad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFVcvseOVC3R"
      },
      "source": [
        "## Set dataset config values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Grb0EeRqVC3R"
      },
      "outputs": [],
      "source": [
        "# if True, model will load features from cache if file is present, or\n",
        "# create features and dump to cache file if not already present\n",
        "config.model.dataset.use_cache = False\n",
        "\n",
        "# indicates whether the dataset has unanswerable questions\n",
        "config.model.dataset.version_2_with_negative = True\n",
        "\n",
        "# indicates whether the dataset is of extractive nature or not\n",
        "# if True, context spans/chunks that do not contain answer are treated as unanswerable\n",
        "config.model.dataset.check_if_answer_in_context = True\n",
        "\n",
        "# set file paths for train, validation, and test datasets\n",
        "config.model.train_ds.file = f\"{DATA_DIR}/squad/v2.0/train-v2.0.json\"\n",
        "config.model.validation_ds.file = f\"{DATA_DIR}/squad/v2.0/dev-v2.0.json\"\n",
        "config.model.test_ds.file = f\"{DATA_DIR}/squad/v2.0/dev-v2.0.json\"\n",
        "\n",
        "# set batch sizes for train, validation, and test datasets\n",
        "config.model.train_ds.batch_size = 8\n",
        "config.model.validation_ds.batch_size = 8\n",
        "config.model.test_ds.batch_size = 8\n",
        "\n",
        "# set number of samples to be used from dataset. setting to -1 uses entire dataset\n",
        "config.model.train_ds.num_samples = 5000\n",
        "config.model.validation_ds.num_samples = 1000\n",
        "config.model.test_ds.num_samples = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFWF41VwVC3R"
      },
      "source": [
        "## Set trainer config values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "42yif-GIVC3R"
      },
      "outputs": [],
      "source": [
        "config.trainer.max_epochs = 1\n",
        "config.trainer.max_steps = -1 # takes precedence over max_epochs\n",
        "config.trainer.precision = 16\n",
        "config.trainer.devices = [0] # 0 for CPU, or list of the GPUs to use [0] this tutorial does not support multiple GPUs. If needed please use NeMo/examples/nlp/question_answering/question_answering.py\n",
        "config.trainer.accelerator = \"gpu\"\n",
        "config.trainer.strategy=\"dp\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDQzMBlbVC3R"
      },
      "source": [
        "## Set experiment manager config values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pxY4rnJBVC3R"
      },
      "outputs": [],
      "source": [
        "config.exp_manager.exp_dir = WORK_DIR\n",
        "config.exp_manager.name = \"QA-SQuAD2\"\n",
        "config.exp_manager.create_wandb_logger=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2_C8reNVC3R"
      },
      "source": [
        "## BERT model for SQuAD v2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Mf-_rioVC3R"
      },
      "source": [
        "### Set model config values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gtlGHzVJVC3R"
      },
      "outputs": [],
      "source": [
        "# set language model and tokenizer to be used\n",
        "# tokenizer is derived from model if a tokenizer name is not provided\n",
        "config.model.language_model.pretrained_model_name = \"bert-base-uncased\"\n",
        "config.model.tokenizer.tokenizer_name = \"bert-base-uncased\"\n",
        "\n",
        "# path where model will be saved\n",
        "config.model.nemo_path = f\"{WORK_DIR}/checkpoints/bert_squad_v2_0.nemo\"\n",
        "\n",
        "config.exp_manager.create_checkpoint_callback = True\n",
        "\n",
        "config.model.optim.lr = 3e-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaM7fe8rVC3R"
      },
      "source": [
        "### Create trainer and initialize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ukLzGmy9VC3R"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(**config.trainer)\n",
        "model = BERTQAModel(config.model, trainer=trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZIA69rlVC3R"
      },
      "source": [
        "### Train, test, and save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "asutB9ZzVC3R",
        "outputId": "f2ab017b-e82e-4615-c999-2866d9498574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "146eaf002c9c4dfeae7ce85d41cb84e4",
            "f2ef09e50d5248b59cca2c76a89be601",
            "0d3731ef06944213b6a364ad123b1990",
            "07b2bdf19d7a4bea9883e1810e7b56c1",
            "162a876df7a84a63accfa63af30a3262",
            "3105b61cafdd4464b899d5a892701775",
            "99bffadd561c453b851a813b51266abf",
            "f08f42fcfde143689c2e54d7990478ca",
            "b66ded7e59a64c4797988dac65df8b4d",
            "8046f28366904f9fb23b7612bade7b85",
            "16866c3ab35c4c2daa8c645a65c9f6c2",
            "5140bd5637774cdcb773a65886741d68",
            "4de338985d8940b4926173ff80384763",
            "eca33a621b36452584fb74dd15aecea1",
            "19aff28321a7447ba633ebd6760cfaed",
            "e1ac4f88197d42319cf821ccded6ec72",
            "60b7436fd39d4aff95ae393e2fa67e1c",
            "77d1524d67a64531b46c87012d84ecbb",
            "0e32aab5615d40f3ac4743c87ecf6ae1",
            "16fb2bf05b5e4c348f4eeb98d1af61e3",
            "07638832d5b8490c88ce479d7fba7743",
            "b5c27265cce24638b1cf6f43a7fa2d40",
            "0e57832ca3024f65805d36ab58d94b07",
            "682dfa95bb044cd4b2df19dc97c09342",
            "ba152217721b484baa47e0ee246e1f06",
            "071782bc8d8d48ce88d054e3a45ade13",
            "5c9b33da695c4c58ae1026be938f47c7",
            "8ddf8f140fff4d1eb0c95eb6b51b1ab0",
            "468bfad660ff4d07a0ffe70bf09b824c",
            "952fffdc746f4b0bb75bd56f60d52d07",
            "4f240868af444c2cbc169c7d14a69abc",
            "e0362a5d08e54db984c0b7beace1ae71",
            "68a80dc04d0b461297df8be8f3fd93f1"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-10-10 17:14:36 modelPT:721] Optimizer config = AdamW (\n",
            "    Parameter Group 0\n",
            "        amsgrad: False\n",
            "        betas: [0.9, 0.999]\n",
            "        capturable: False\n",
            "        differentiable: False\n",
            "        eps: 1e-08\n",
            "        foreach: None\n",
            "        fused: None\n",
            "        lr: 3e-05\n",
            "        maximize: False\n",
            "        weight_decay: 0.0\n",
            "    )\n",
            "[NeMo I 2023-10-10 17:14:36 lr_scheduler:910] Scheduler \"<nemo.core.optim.lr_scheduler.SquareRootAnnealing object at 0x7b314764af50>\" \n",
            "    will be used during training (effective maximum steps = 628) - \n",
            "    Parameters : \n",
            "    (warmup_steps: null\n",
            "    warmup_ratio: 0.0\n",
            "    last_epoch: -1\n",
            "    max_steps: 628\n",
            "    )\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name       | Type            | Params\n",
            "-----------------------------------------------\n",
            "0 | bert_model | BertEncoder     | 109 M \n",
            "1 | classifier | TokenClassifier | 1.5 K \n",
            "2 | loss       | SpanningLoss    | 0     \n",
            "-----------------------------------------------\n",
            "109 M     Trainable params\n",
            "0         Non-trainable params\n",
            "109 M     Total params\n",
            "218.968   Total estimated model params size (MB)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "146eaf002c9c4dfeae7ce85d41cb84e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2023-10-10 17:14:37 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/core/module.py:481: UserWarning: You called `self.log('train_loss', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
            "      rank_zero_warn(\n",
            "    \n",
            "[NeMo W 2023-10-10 17:14:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:139: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "      warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "    \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Validation: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5140bd5637774cdcb773a65886741d68"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-10-10 17:18:25 qa_bert_model:140] val exact: 29.1\n",
            "[NeMo I 2023-10-10 17:18:25 qa_bert_model:140] val f1: 34.41956674569356\n",
            "[NeMo I 2023-10-10 17:18:25 qa_bert_model:140] val total: 1000.0\n",
            "[NeMo I 2023-10-10 17:18:25 qa_bert_model:140] val HasAns_exact: 58.032128514056225\n",
            "[NeMo I 2023-10-10 17:18:25 qa_bert_model:140] val HasAns_f1: 68.71398944918384\n",
            "[NeMo I 2023-10-10 17:18:25 qa_bert_model:140] val HasAns_total: 498.0\n",
            "[NeMo I 2023-10-10 17:18:25 qa_bert_model:140] val NoAns_exact: 0.398406374501992\n",
            "[NeMo I 2023-10-10 17:18:25 qa_bert_model:140] val NoAns_f1: 0.398406374501992\n",
            "[NeMo I 2023-10-10 17:18:25 qa_bert_model:140] val NoAns_total: 502.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=1` reached.\n",
            "INFO:pytorch_lightning.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Testing: 0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e57832ca3024f65805d36ab58d94b07"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-10-10 17:18:29 qa_bert_model:140] test exact: 30.0\n",
            "[NeMo I 2023-10-10 17:18:29 qa_bert_model:140] test f1: 33.999633699633705\n",
            "[NeMo I 2023-10-10 17:18:29 qa_bert_model:140] test total: 100.0\n",
            "[NeMo I 2023-10-10 17:18:29 qa_bert_model:140] test HasAns_exact: 66.66666666666667\n",
            "[NeMo I 2023-10-10 17:18:29 qa_bert_model:140] test HasAns_f1: 75.55474155474157\n",
            "[NeMo I 2023-10-10 17:18:29 qa_bert_model:140] test HasAns_total: 45.0\n",
            "[NeMo I 2023-10-10 17:18:29 qa_bert_model:140] test NoAns_exact: 0.0\n",
            "[NeMo I 2023-10-10 17:18:29 qa_bert_model:140] test NoAns_f1: 0.0\n",
            "[NeMo I 2023-10-10 17:18:29 qa_bert_model:140] test NoAns_total: 55.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│\u001b[36m \u001b[0m\u001b[36m    test_HasAns_exact    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    66.66666412353516    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m     test_HasAns_f1      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    75.55474090576172    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m    test_HasAns_total    \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          45.0           \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m    test_NoAns_exact     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m      test_NoAns_f1      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m           0.0           \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m    test_NoAns_total     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          55.0           \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m       test_exact        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          30.0           \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m         test_f1         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    33.9996337890625     \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    5.231266021728516    \u001b[0m\u001b[35m \u001b[0m│\n",
              "│\u001b[36m \u001b[0m\u001b[36m       test_total        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m          100.0          \u001b[0m\u001b[35m \u001b[0m│\n",
              "└───────────────────────────┴───────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">     test_HasAns_exact     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     66.66666412353516     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">      test_HasAns_f1       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     75.55474090576172     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">     test_HasAns_total     </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           45.0            </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">     test_NoAns_exact      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_NoAns_f1       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">            0.0            </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">     test_NoAns_total      </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           55.0            </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_exact         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           30.0            </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">          test_f1          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     33.9996337890625      </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">         test_loss         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     5.231266021728516     </span>│\n",
              "│<span style=\"color: #008080; text-decoration-color: #008080\">        test_total         </span>│<span style=\"color: #800080; text-decoration-color: #800080\">           100.0           </span>│\n",
              "└───────────────────────────┴───────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer.fit(model)\n",
        "trainer.test(model)\n",
        "\n",
        "model.save_to(config.model.nemo_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5AIv0SEVC3S"
      },
      "source": [
        "### Load the saved model and run inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "7k5kD6tvVC3S",
        "outputId": "e82c4e7d-96b5-4525-c851-5efe94a975e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-10-10 17:18:43 tokenizer_utils:130] Getting HuggingFace AutoTokenizer with pretrained_model_name: bert-base-uncased, vocab_file: /tmp/tmpi1tib6ds/a074c300b8564899b27a83dc7410935e_vocab.txt, merges_files: None, special_tokens_dict: {}, and use_fast: False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using eos_token, but it is not set yet.\n",
            "Using bos_token, but it is not set yet.\n",
            "[NeMo W 2023-10-10 17:18:44 modelPT:244] You tried to register an artifact under config key=tokenizer.vocab_file but an artifact for it has already been registered.\n",
            "[NeMo W 2023-10-10 17:18:44 modelPT:161] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    file: /content/Data/squad/v2.0/train-v2.0.json\n",
            "    batch_size: 8\n",
            "    shuffle: true\n",
            "    num_samples: 5000\n",
            "    num_workers: 1\n",
            "    drop_last: false\n",
            "    pin_memory: false\n",
            "    \n",
            "[NeMo W 2023-10-10 17:18:44 modelPT:168] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    file: /content/Data/squad/v2.0/dev-v2.0.json\n",
            "    batch_size: 8\n",
            "    shuffle: false\n",
            "    num_samples: 1000\n",
            "    num_workers: 1\n",
            "    drop_last: false\n",
            "    pin_memory: false\n",
            "    \n",
            "[NeMo W 2023-10-10 17:18:44 modelPT:174] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    file: /content/Data/squad/v2.0/dev-v2.0.json\n",
            "    batch_size: 8\n",
            "    shuffle: false\n",
            "    num_samples: 100\n",
            "    num_workers: 1\n",
            "    drop_last: false\n",
            "    pin_memory: false\n",
            "    \n",
            "[NeMo W 2023-10-10 17:18:44 nlp_overrides:253] Apex was not found. Please see the NeMo README for installation instructions: https://github.com/NVIDIA/apex\n",
            "    Megatron-based models require Apex to function correctly.\n",
            "[NeMo W 2023-10-10 17:18:52 modelPT:244] You tried to register an artifact under config key=language_model.config_file but an artifact for it has already been registered.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-10-10 17:18:53 save_restore_connector:249] Model BERTQAModel was successfully restored from /content/Model/checkpoints/bert_squad_v2_0.nemo.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:Using 16bit None Automatic Mixed Precision (AMP)\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: True (cuda), used: True\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:IPU available: False, using: 0 IPUs\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2023-10-10 17:18:53 exp_manager:374] Experiments will be logged at /content/Model/QA-SQuAD2/2023-10-10_17-18-53\n",
            "[NeMo I 2023-10-10 17:18:53 exp_manager:797] TensorboardLogger has been set up\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 166.76it/s]\n",
            "100%|██████████| 10/10 [00:00<00:00, 66470.74it/s]\n",
            "100%|██████████| 10/10 [00:00<00:00, 35544.95it/s]\n",
            "100%|██████████| 10/10 [00:00<00:00, 8055.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "France\n",
            "10th and 11th centuries\n",
            "Denmark, Iceland and Norway\n",
            "Rollo\n",
            "10th century\n",
            "The Normans\n",
            "Normandy\n",
            "Rollo\n",
            "10th century\n",
            "William the Conqueror\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model = BERTQAModel.restore_from(config.model.nemo_path)\n",
        "\n",
        "eval_device = [config.trainer.devices[0]] if isinstance(config.trainer.devices, list) else 1\n",
        "model.trainer = pl.Trainer(\n",
        "    devices=eval_device,\n",
        "    accelerator=config.trainer.accelerator,\n",
        "    precision=16,\n",
        "    logger=False,\n",
        ")\n",
        "\n",
        "config.exp_manager.create_checkpoint_callback = False\n",
        "exp_dir = exp_manager(model.trainer, config.exp_manager)\n",
        "output_nbest_file = os.path.join(exp_dir, \"output_nbest_file.json\")\n",
        "output_prediction_file = os.path.join(exp_dir, \"output_prediction_file.json\")\n",
        "\n",
        "all_preds, all_nbest = model.inference(\n",
        "    config.model.test_ds.file,\n",
        "    output_prediction_file=output_prediction_file,\n",
        "    output_nbest_file=output_nbest_file,\n",
        "    num_samples=10, # setting to -1 will use all samples for inference\n",
        ")\n",
        "\n",
        "for question_id in all_preds:\n",
        "    print(all_preds[question_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyh0SNiyVC3S"
      },
      "source": [
        "## S2S BART model for SQuAD v2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sy9IYgVYVC3S"
      },
      "source": [
        "### Set model config values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKNmHKV5VC3S"
      },
      "outputs": [],
      "source": [
        "# set language model and tokenizer to be used\n",
        "# tokenizer is derived from model if a tokenizer name is not provided\n",
        "config.model.language_model.pretrained_model_name = \"facebook/bart-base\"\n",
        "config.model.tokenizer.tokenizer_name = \"facebook/bart-base\"\n",
        "\n",
        "# path where model will be saved\n",
        "config.model.nemo_path = f\"{WORK_DIR}/checkpoints/bart_squad_v2_0.nemo\"\n",
        "\n",
        "config.exp_manager.create_checkpoint_callback = True\n",
        "\n",
        "config.model.optim.lr = 5e-5\n",
        "\n",
        "#remove vocab_file from gpt model\n",
        "config.model.tokenizer.vocab_file = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_0glS4yVC3S"
      },
      "source": [
        "### Create trainer and initialize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jWyHY1oVC3S"
      },
      "outputs": [],
      "source": [
        "# uncomment below line and run if you get an error while initializing tokenizer on Colab (reference: https://github.com/huggingface/transformers/issues/8690)\n",
        "# !rm -r /root/.cache/huggingface/\n",
        "\n",
        "trainer = pl.Trainer(**config.trainer)\n",
        "model = S2SQAModel(config.model, trainer=trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg-j39b4VC3S"
      },
      "source": [
        "### Train, test, and save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocsf0EBDVC3S"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model)\n",
        "trainer.test(model)\n",
        "\n",
        "model.save_to(config.model.nemo_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vs3pl0VMVC3S"
      },
      "source": [
        "### Load the saved model and run inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoW6_GO_VC3S"
      },
      "outputs": [],
      "source": [
        "model = S2SQAModel.restore_from(config.model.nemo_path)\n",
        "\n",
        "eval_device = [config.trainer.devices[0]] if isinstance(config.trainer.devices, list) else 1\n",
        "model.trainer = pl.Trainer(\n",
        "    devices=eval_device,\n",
        "    accelerator=config.trainer.accelerator,\n",
        "    precision=16,\n",
        "    logger=False,\n",
        ")\n",
        "\n",
        "config.exp_manager.create_checkpoint_callback = False\n",
        "exp_dir = exp_manager(model.trainer, config.exp_manager)\n",
        "output_nbest_file = os.path.join(exp_dir, \"output_nbest_file.json\")\n",
        "output_prediction_file = os.path.join(exp_dir, \"output_prediction_file.json\")\n",
        "\n",
        "all_preds, all_nbest = model.inference(\n",
        "    config.model.test_ds.file,\n",
        "    output_prediction_file=output_prediction_file,\n",
        "    output_nbest_file=output_nbest_file,\n",
        "    num_samples=10, # setting to -1 will use all samples for inference\n",
        ")\n",
        "\n",
        "for question_id in all_preds:\n",
        "    print(all_preds[question_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7-iInbPVC3S"
      },
      "source": [
        "## GPT2 model for SQuAD v2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaIC0l2aVC3S"
      },
      "source": [
        "### Set model config values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5j6SVk6fVC3S"
      },
      "outputs": [],
      "source": [
        "# set language model and tokenizer to be used\n",
        "# tokenizer is derived from model if a tokenizer name is not provided\n",
        "config.model.language_model.pretrained_model_name = \"gpt2\"\n",
        "config.model.tokenizer.tokenizer_name = \"gpt2\"\n",
        "\n",
        "# path where model will be saved\n",
        "config.model.nemo_path = f\"{WORK_DIR}/checkpoints/gpt2_squad_v2_0.nemo\"\n",
        "\n",
        "config.exp_manager.create_checkpoint_callback = True\n",
        "\n",
        "config.model.optim.lr = 1e-4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWhhEuvzVC3S"
      },
      "source": [
        "### Create trainer and initialize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBtP3ukDVC3S"
      },
      "outputs": [],
      "source": [
        "# uncomment below line and run if you get an error while initializing tokenizer on Colab (reference: https://github.com/huggingface/transformers/issues/8690)\n",
        "# !rm -r /root/.cache/huggingface/\n",
        "\n",
        "trainer = pl.Trainer(**config.trainer)\n",
        "model = GPTQAModel(config.model, trainer=trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EApFrJh8VC3T"
      },
      "source": [
        "### Train, test, and save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYo2JDdOVC3T"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model)\n",
        "trainer.test(model)\n",
        "\n",
        "model.save_to(config.model.nemo_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aNEt06fVC3T"
      },
      "source": [
        "### Load the saved model and run inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioLT4DVbVC3T"
      },
      "outputs": [],
      "source": [
        "model = GPTQAModel.restore_from(config.model.nemo_path)\n",
        "\n",
        "eval_device = [config.trainer.devices[0]] if isinstance(config.trainer.devices, list) else 1\n",
        "model.trainer = pl.Trainer(\n",
        "    devices=eval_device,\n",
        "    accelerator=config.trainer.accelerator,\n",
        "    precision=16,\n",
        "    logger=False,\n",
        ")\n",
        "\n",
        "config.exp_manager.create_checkpoint_callback = False\n",
        "exp_dir = exp_manager(model.trainer, config.exp_manager)\n",
        "output_nbest_file = os.path.join(exp_dir, \"output_nbest_file.json\")\n",
        "output_prediction_file = os.path.join(exp_dir, \"output_prediction_file.json\")\n",
        "\n",
        "all_preds, all_nbest = model.inference(\n",
        "    config.model.test_ds.file,\n",
        "    output_prediction_file=output_prediction_file,\n",
        "    output_nbest_file=output_nbest_file,\n",
        "    num_samples=10, # setting to -1 will use all samples for inference\n",
        ")\n",
        "\n",
        "for question_id in all_preds:\n",
        "    print(all_preds[question_id])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTWOlD9AVC3T"
      },
      "source": [
        "# Training and testing models on MS-MARCO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZWsMwnGVC3T"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRUAwgAbVC3T"
      },
      "source": [
        "### Downloading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qz3DO9JGVC3T"
      },
      "source": [
        "MS-MARCO(Microsoft Machine Reading Comprehension) is a large scale dataset focused on machine reading comprehension, question answering, and passage ranking. MS-MARCO consists of 1,010,916 queries generated from real, anonymized Bing user queries. The contexts are extracted from real web documents and the answers are generated by humans.\n",
        "\n",
        "Please agree to the Terms of Use at https://microsoft.github.io/msmarco/ before downloading the data\n",
        "\n",
        "The data can be downloaded at:\n",
        "- https://msmarco.blob.core.windows.net/msmarco/train_v2.1.json.gz\n",
        "- https://msmarco.blob.core.windows.net/msmarco/dev_v2.1.json.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fm5MzZ91inP5"
      },
      "outputs": [],
      "source": [
        "os.makedirs(os.path.join(DATA_DIR, \"msmarco\"), exist_ok=True)\n",
        "\n",
        "!wget https://msmarco.blob.core.windows.net/msmarco/train_v2.1.json.gz -P $DATA_DIR/msmarco\n",
        "!gunzip $DATA_DIR/msmarco/train_v2.1.json.gz\n",
        "\n",
        "!wget https://msmarco.blob.core.windows.net/msmarco/dev_v2.1.json.gz -P $DATA_DIR/msmarco\n",
        "!gunzip $DATA_DIR/msmarco/dev_v2.1.json.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDmFHzBtVC3T"
      },
      "source": [
        "### Converting to SQuAD format\n",
        "\n",
        "The script for converting MS-MARCO dataset to SQuAD can be found at `NeMo/examples/nlp/question_answering/convert_msmarco_to_squad_format.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJtNIzZQVC3T"
      },
      "outputs": [],
      "source": [
        "# download convert_msmarco_to_squad_format.py script to format the MS-MARCO data\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "if not os.path.exists(WORK_DIR + '/convert_msmarco_to_squad_format.py'):\n",
        "    print('Downloading convert_msmarco_to_squad_format.py...')\n",
        "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/question_answering/convert_msmarco_to_squad_format.py', WORK_DIR)\n",
        "else:\n",
        "    print ('convert_msmarco_to_squad_format.py already exists')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Io_esJPSuBcW"
      },
      "outputs": [],
      "source": [
        "# we will exclude examples from MS-MARCO dataset that do not have a wellFormedAnswer using a utility script\n",
        "# download remove_ms_marco_samples_without_wellFormedAnswers.py script to format the MS-MARCO data\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "if not os.path.exists(WORK_DIR + '/remove_ms_marco_samples_without_wellFormedAnswers.py'):\n",
        "    print('Downloading remove_ms_marco_samples_without_wellFormedAnswers.py...')\n",
        "    wget.download(f'https://raw.githubusercontent.com/NVIDIA/NeMo/{BRANCH}/examples/nlp/dialogue/remove_ms_marco_samples_without_wellFormedAnswers.py', WORK_DIR)\n",
        "else:\n",
        "    print ('remove_ms_marco_samples_without_wellFormedAnswers.py already exists')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cs_CXkfXuYVQ"
      },
      "outputs": [],
      "source": [
        "!python $WORK_DIR/remove_ms_marco_samples_without_wellFormedAnswers.py --filename $DATA_DIR/msmarco/train_v2.1.json\n",
        "!python $WORK_DIR/remove_ms_marco_samples_without_wellFormedAnswers.py --filename $DATA_DIR/msmarco/dev_v2.1.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUAKI086VC3T"
      },
      "outputs": [],
      "source": [
        "!(python $WORK_DIR/convert_msmarco_to_squad_format.py \\\n",
        "    --msmarco_train_input_filepath=$DATA_DIR/msmarco/train_v2.1.json \\\n",
        "    --msmarco_dev_input_filepath=$DATA_DIR/msmarco/dev_v2.1.json \\\n",
        "    --converted_train_save_path=$DATA_DIR/msmarco/msmarco-squad-format-train-v2.1.json \\\n",
        "    --converted_dev_save_path=$DATA_DIR/msmarco/msmarco-squad-format-dev-v2.1.json \\\n",
        "    --exclude_negative_samples=False \\\n",
        "    --keep_only_relevant_passages=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeHesaFcVC3T"
      },
      "source": [
        "## Set dataset config values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhx-_1X3VC3T"
      },
      "outputs": [],
      "source": [
        "# if True, model will load features from cache if file is present, or\n",
        "# create features and dump to cache file if not already present\n",
        "config.model.dataset.use_cache = False\n",
        "\n",
        "# indicates whether the dataset has unanswerable questions\n",
        "config.model.dataset.version_2_with_negative = True\n",
        "\n",
        "# if True, context spans/chunks that do not contain answer are treated as unanswerable\n",
        "# should be False for MS-MARCO dataset, or other datasets of generative nature\n",
        "config.model.dataset.check_if_answer_in_context = False\n",
        "\n",
        "# set file paths for train, validation, and test datasets\n",
        "config.model.train_ds.file = f\"{DATA_DIR}/msmarco/msmarco-squad-format-train-v2.1.json\"\n",
        "config.model.validation_ds.file = f\"{DATA_DIR}/msmarco/msmarco-squad-format-dev-v2.1.json\"\n",
        "config.model.test_ds.file = f\"{DATA_DIR}/msmarco/msmarco-squad-format-dev-v2.1.json\"\n",
        "\n",
        "# set batch sizes for train, validation, and test datasets\n",
        "config.model.train_ds.batch_size = 16\n",
        "config.model.validation_ds.batch_size = 16\n",
        "config.model.test_ds.batch_size = 16\n",
        "\n",
        "# set number of samples to be used from dataset. setting to -1 uses entire dataset\n",
        "config.model.train_ds.num_samples = 5000\n",
        "config.model.validation_ds.num_samples = 1000\n",
        "config.model.test_ds.num_samples = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X43k_EeqVC3T"
      },
      "source": [
        "## Set trainer config values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HavpkQLPVC3U"
      },
      "outputs": [],
      "source": [
        "config.trainer.max_epochs = 1\n",
        "config.trainer.max_steps = -1 # takes precedence over max_epochs\n",
        "config.trainer.precision = 16\n",
        "config.trainer.devices = [0] # 0 for CPU, or list of the GPUs to use e.g. [0, 1] or [0]\n",
        "config.trainer.accelerator = \"gpu\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-_FIZE2VC3U"
      },
      "source": [
        "## Set experiment manager config values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10TT3okiVC3U"
      },
      "outputs": [],
      "source": [
        "config.exp_manager.exp_dir = WORK_DIR\n",
        "config.exp_manager.name = \"QA-MSMARCO\"\n",
        "config.exp_manager.create_wandb_logger=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKIq6YT-VC3U"
      },
      "source": [
        "## S2S BART model for MS-MARCO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvf-QpYLVC3U"
      },
      "source": [
        "### Set model config values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDVZ1a5fVC3U"
      },
      "outputs": [],
      "source": [
        "# set language model and tokenizer to be used\n",
        "# tokenizer is derived from model if a tokenizer name is not provided\n",
        "config.model.language_model.pretrained_model_name = \"facebook/bart-base\"\n",
        "config.model.tokenizer.tokenizer_name = \"facebook/bart-base\"\n",
        "\n",
        "# path where model will be saved\n",
        "config.model.nemo_path = f\"{WORK_DIR}/checkpoints/bart_msmarco_v2_0.nemo\"\n",
        "\n",
        "config.exp_manager.create_checkpoint_callback = True\n",
        "\n",
        "config.model.optim.lr = 5e-5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N75cdLRVC3U"
      },
      "source": [
        "### Create trainer and initialize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv9UMkfxVC3U"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(**config.trainer)\n",
        "model = S2SQAModel(config.model, trainer=trainer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhVuV9sWVC3U"
      },
      "source": [
        "### Train, test, and save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JeaJ_OgVC3U"
      },
      "outputs": [],
      "source": [
        "trainer.fit(model)\n",
        "trainer.test(model)\n",
        "\n",
        "model.save_to(config.model.nemo_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj0dGexaVC3U"
      },
      "source": [
        "### Load the saved model and run inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1elN-WDVC3U"
      },
      "outputs": [],
      "source": [
        "model = S2SQAModel.restore_from(config.model.nemo_path)\n",
        "\n",
        "eval_device = [config.trainer.devices[0]] if isinstance(config.trainer.devices, list) else 1\n",
        "model.trainer = pl.Trainer(\n",
        "    devices=eval_device,\n",
        "    accelerator=config.trainer.accelerator,\n",
        "    precision=16,\n",
        "    logger=False,\n",
        ")\n",
        "\n",
        "config.exp_manager.create_checkpoint_callback = False\n",
        "exp_dir = exp_manager(model.trainer, config.exp_manager)\n",
        "output_nbest_file = os.path.join(exp_dir, \"output_nbest_file.json\")\n",
        "output_prediction_file = os.path.join(exp_dir, \"output_prediction_file.json\")\n",
        "\n",
        "all_preds, all_nbest = model.inference(\n",
        "    config.model.test_ds.file,\n",
        "    output_prediction_file=output_prediction_file,\n",
        "    output_nbest_file=output_nbest_file,\n",
        "    num_samples=10, # setting to -1 will use all samples for inference\n",
        ")\n",
        "\n",
        "for question_id in all_preds:\n",
        "    print(all_preds[question_id])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Question_Answering.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.0 ('test_ptl_1.7')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e987a19b1bc60996a600adb5d563aa4a4c022e7b31abb2e65c324714934e8ea9"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "146eaf002c9c4dfeae7ce85d41cb84e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2ef09e50d5248b59cca2c76a89be601",
              "IPY_MODEL_0d3731ef06944213b6a364ad123b1990",
              "IPY_MODEL_07b2bdf19d7a4bea9883e1810e7b56c1"
            ],
            "layout": "IPY_MODEL_162a876df7a84a63accfa63af30a3262"
          }
        },
        "f2ef09e50d5248b59cca2c76a89be601": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3105b61cafdd4464b899d5a892701775",
            "placeholder": "​",
            "style": "IPY_MODEL_99bffadd561c453b851a813b51266abf",
            "value": "Epoch 0: 100%"
          }
        },
        "0d3731ef06944213b6a364ad123b1990": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f08f42fcfde143689c2e54d7990478ca",
            "max": 753,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b66ded7e59a64c4797988dac65df8b4d",
            "value": 753
          }
        },
        "07b2bdf19d7a4bea9883e1810e7b56c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8046f28366904f9fb23b7612bade7b85",
            "placeholder": "​",
            "style": "IPY_MODEL_16866c3ab35c4c2daa8c645a65c9f6c2",
            "value": " 753/753 [03:49&lt;00:00,  3.29it/s, loss=1.56, lr=1.2e-6, train_loss_step=2.040, train_loss_epoch=2.240]"
          }
        },
        "162a876df7a84a63accfa63af30a3262": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "3105b61cafdd4464b899d5a892701775": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99bffadd561c453b851a813b51266abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f08f42fcfde143689c2e54d7990478ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b66ded7e59a64c4797988dac65df8b4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8046f28366904f9fb23b7612bade7b85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16866c3ab35c4c2daa8c645a65c9f6c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5140bd5637774cdcb773a65886741d68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4de338985d8940b4926173ff80384763",
              "IPY_MODEL_eca33a621b36452584fb74dd15aecea1",
              "IPY_MODEL_19aff28321a7447ba633ebd6760cfaed"
            ],
            "layout": "IPY_MODEL_e1ac4f88197d42319cf821ccded6ec72"
          }
        },
        "4de338985d8940b4926173ff80384763": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60b7436fd39d4aff95ae393e2fa67e1c",
            "placeholder": "​",
            "style": "IPY_MODEL_77d1524d67a64531b46c87012d84ecbb",
            "value": "Validation DataLoader 0: 100%"
          }
        },
        "eca33a621b36452584fb74dd15aecea1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e32aab5615d40f3ac4743c87ecf6ae1",
            "max": 125,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_16fb2bf05b5e4c348f4eeb98d1af61e3",
            "value": 125
          }
        },
        "19aff28321a7447ba633ebd6760cfaed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07638832d5b8490c88ce479d7fba7743",
            "placeholder": "​",
            "style": "IPY_MODEL_b5c27265cce24638b1cf6f43a7fa2d40",
            "value": " 125/125 [00:11&lt;00:00, 10.78it/s]"
          }
        },
        "e1ac4f88197d42319cf821ccded6ec72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "60b7436fd39d4aff95ae393e2fa67e1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77d1524d67a64531b46c87012d84ecbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e32aab5615d40f3ac4743c87ecf6ae1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16fb2bf05b5e4c348f4eeb98d1af61e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "07638832d5b8490c88ce479d7fba7743": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5c27265cce24638b1cf6f43a7fa2d40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e57832ca3024f65805d36ab58d94b07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_682dfa95bb044cd4b2df19dc97c09342",
              "IPY_MODEL_ba152217721b484baa47e0ee246e1f06",
              "IPY_MODEL_071782bc8d8d48ce88d054e3a45ade13"
            ],
            "layout": "IPY_MODEL_5c9b33da695c4c58ae1026be938f47c7"
          }
        },
        "682dfa95bb044cd4b2df19dc97c09342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ddf8f140fff4d1eb0c95eb6b51b1ab0",
            "placeholder": "​",
            "style": "IPY_MODEL_468bfad660ff4d07a0ffe70bf09b824c",
            "value": "Testing DataLoader 0: 100%"
          }
        },
        "ba152217721b484baa47e0ee246e1f06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_952fffdc746f4b0bb75bd56f60d52d07",
            "max": 13,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4f240868af444c2cbc169c7d14a69abc",
            "value": 13
          }
        },
        "071782bc8d8d48ce88d054e3a45ade13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0362a5d08e54db984c0b7beace1ae71",
            "placeholder": "​",
            "style": "IPY_MODEL_68a80dc04d0b461297df8be8f3fd93f1",
            "value": " 13/13 [00:01&lt;00:00,  8.02it/s]"
          }
        },
        "5c9b33da695c4c58ae1026be938f47c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "8ddf8f140fff4d1eb0c95eb6b51b1ab0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "468bfad660ff4d07a0ffe70bf09b824c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "952fffdc746f4b0bb75bd56f60d52d07": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f240868af444c2cbc169c7d14a69abc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0362a5d08e54db984c0b7beace1ae71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68a80dc04d0b461297df8be8f3fd93f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}